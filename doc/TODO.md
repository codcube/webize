# Model
- offline/online content merge - following feeds (API or RSS/Atom) to canonical URL may result in empty page due to SPA stuff, but we have some content already. plus the index/known-dir structures added in when live remote browsing. and peer merge: - ask all the peer-caches (pi/vps/phone from laptop) ahead of origin servers for static resources - HEAD All then cancel remaining HEADs on first response and GET winner? or announce availability at cache-time of stuff not autoimagically syndicated (larger static media stuff etc)
- define webresource level #join and call super/RDF::URI #join to eliminate manual environment threading. mild 'code could be more simple' improvements like this 
- history - store versions
- backlink/inbound-arc + geo indexing - sort results by frequently linked posts

# HTTP(S)
- auto-certgen for Falcon for full MITM for data-archival while using upstream UIs and mobile apps (See localhost gem as starting point)
- ruby/falcon/async/websocket streaming for earlier first byte on multi/merge-GET and update-syndication for local cluster/mesh
- #fetchList populated by outbound links (load all search results)

# Format
- path visualization (how did i get here? referer logging / backlink history)
- webize git
- timeline-histogram view
